{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5azJbrIqXAZ"
      },
      "source": [
        "# NDT Internet Speed Prediction using Deep Learning\n",
        "\n",
        "**Author:** Wen Bo Li and Patricia Wei\n",
        "\n",
        "**Date:** May 19, 2023\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoyrBnCRMwp1"
      },
      "source": [
        "# Set up authentication\n",
        "\n",
        "Run this to approve Colab to use your Google credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ay72u8IFMy28",
        "outputId": "b302393c-5a8c-41a6-a9b8-6bf070d4d58a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.5.3\n"
          ]
        }
      ],
      "source": [
        "try: auth\n",
        "except: \n",
        "  !apt update\n",
        "  !pip install pandas google-cloud-storage\n",
        "\n",
        "  import pandas as pd\n",
        "  from google.colab import auth\n",
        "\n",
        "print (pd.__version__)\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF4COhJ_lSFc"
      },
      "source": [
        "# Getting the Raw Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf0gYZcKaXO0"
      },
      "source": [
        "## Imports/utilities for Queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpSTyaWjaZi6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import re\n",
        "import statistics\n",
        "from tabulate import tabulate\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from matplotlib.ticker import FormatStrFormatter\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "from datetime import datetime\n",
        "from datetime import date\n",
        "from datetime import timedelta\n",
        "from dateutil.parser import parse as dateparser\n",
        "import pytz\n",
        "from pytz import timezone\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "utc_tz = pytz.timezone('UTC')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL5OWEkpG62X"
      },
      "source": [
        "## Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ECPRom_G8Yb"
      },
      "outputs": [],
      "source": [
        "def get_dists(df, col):\n",
        "  # Frequency\n",
        "  stats_df = df \\\n",
        "  .groupby(col) \\\n",
        "  [col] \\\n",
        "  .agg('count') \\\n",
        "  .pipe(pd.DataFrame) \\\n",
        "  .rename(columns = {col: 'frequency'})\n",
        "\n",
        "  # PDF\n",
        "  stats_df['pdf'] = stats_df['frequency'] / sum(stats_df['frequency'])\n",
        "\n",
        "  # CDF\n",
        "  stats_df['cdf'] = stats_df['pdf'].cumsum()\n",
        "  stats_df = stats_df.reset_index()\n",
        "  return stats_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLmCpNZxJv5h"
      },
      "source": [
        "## Query Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvUgkvJLJvYY"
      },
      "outputs": [],
      "source": [
        "start_date = '2023-02-01'\n",
        "end_date = '2023-02-03'\n",
        "\n",
        "start_time = f'{start_date} 00:00:00 UTC'\n",
        "end_time = f'{end_date} 23:59:59 UTC'\n",
        "start_time_dt = datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S UTC')\n",
        "end_time_dt = datetime.strptime(end_time, '%Y-%m-%d %H:%M:%S UTC')\n",
        "\n",
        "num_days = 1\n",
        "min_datapoints = 1\n",
        "min_daily_datapoints = 1\n",
        "min_duration = 1 #seconds\n",
        "\n",
        "mlab_project = 'measurement-lab'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3Pb1aBAUvtt"
      },
      "source": [
        "## Very Small Query Parameters so the query actually finishes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oshjc71yUvRD"
      },
      "outputs": [],
      "source": [
        "start_date = '2023-02-01'\n",
        "end_date = '2023-02-01'\n",
        "\n",
        "start_time = f'{start_date} 00:00:00 UTC'\n",
        "end_time = f'{end_date} 5:59:59 UTC'\n",
        "start_time_dt = datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S UTC')\n",
        "end_time_dt = datetime.strptime(end_time, '%Y-%m-%d %H:%M:%S UTC')\n",
        "\n",
        "num_days = 1\n",
        "min_datapoints = 1\n",
        "min_daily_datapoints = 1\n",
        "min_duration = 1 #seconds\n",
        "\n",
        "mlab_project = 'measurement-lab'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7jbhh4Xlp38"
      },
      "source": [
        "## Query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkNSL3mdkPGm"
      },
      "source": [
        "### Subqueries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8z3OeS0kRTw"
      },
      "outputs": [],
      "source": [
        "def get_bytes_sample(bytes_MB):\n",
        "  return f'''\n",
        "bytes_sample_{bytes_MB}MB AS (\n",
        "  SELECT\n",
        "    UUID,\n",
        "    ARRAY (\n",
        "      SELECT\n",
        "        AS STRUCT *\n",
        "      FROM\n",
        "        UNNEST(Snapshots)\n",
        "      WHERE\n",
        "        BytesAcked < {bytes_MB*1e6}\n",
        "      ORDER BY\n",
        "        BytesAcked DESC\n",
        "    ) AS Snapshots_sample\n",
        "  FROM\n",
        "    preprocessed\n",
        "),\n",
        "bytes_cum_sample_{bytes_MB}MB AS (\n",
        "  SELECT\n",
        "    UUID,\n",
        "  IF\n",
        "    (ARRAY_LENGTH(Snapshots_sample) > 0,\n",
        "        Snapshots_sample[OFFSET(0)].CumAvgMbps, -1) AS Cum_Estimates_{bytes_MB}MB\n",
        "  FROM\n",
        "    bytes_sample_{bytes_MB}MB\n",
        "),\n",
        "bytes_p5_sample_{bytes_MB}MB AS (\n",
        "  WITH\n",
        "    tmp AS (\n",
        "      SELECT\n",
        "        UUID,\n",
        "        APPROX_QUANTILES(CurrAvgMbps, 100)[OFFSET(5)] AS p5\n",
        "      FROM\n",
        "        bytes_sample_{bytes_MB}MB, UNNEST(Snapshots_sample)\n",
        "      GROUP BY\n",
        "        UUID\n",
        "    )\n",
        "  SELECT\n",
        "    bytes_sample_{bytes_MB}MB.UUID,\n",
        "    AVG(CurrAvgMbps) AS Sampled_Estimates_{bytes_MB}MB\n",
        "  FROM\n",
        "    bytes_sample_{bytes_MB}MB, UNNEST(Snapshots_sample)\n",
        "  JOIN\n",
        "    tmp\n",
        "  ON\n",
        "    tmp.UUID = bytes_sample_{bytes_MB}MB.UUID\n",
        "  WHERE\n",
        "    CurrAvgMbps > p5\n",
        "  GROUP BY\n",
        "    1\n",
        ")\n",
        "'''\n",
        "\n",
        "def bytes_inner_join(bytes_MB):\n",
        "  return f'''\n",
        "INNER JOIN bytes_cum_sample_{bytes_MB}MB\n",
        "ON bytes_cum_sample_{bytes_MB}MB.UUID = preprocessed.UUID\n",
        "INNER JOIN bytes_p5_sample_{bytes_MB}MB\n",
        "ON bytes_p5_sample_{bytes_MB}MB.UUID = preprocessed.UUID\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQTsNgmnLSW0"
      },
      "source": [
        "### Original Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfiIQRrdLT0N"
      },
      "outputs": [],
      "source": [
        "def get_data_v3(start_date, end_date, start_time, end_time, min_daily_datapoints, num_days, min_datapoints, min_duration, to_print=False):\n",
        "  query = f'''\n",
        "WITH\n",
        "preprocessed AS (\n",
        "  SELECT\n",
        "    ndt7.raw.Download.UUID AS UUID,\n",
        "    ndt7.raw.ClientIP AS ClientIP,\n",
        "    ndt7.Server.Site AS Site,\n",
        "    ndt7.a.TestTime AS TestTime,\n",
        "    ndt7.a.MeanThroughputMbps AS ReturnedThroughputMbps,\n",
        "    TIMESTAMP_DIFF(ndt7.raw.Download.EndTime, ndt7.raw.Download.StartTime, SECOND) AS Duration_s,\n",
        "    (SELECT max(ServerMeasurements.TCPInfo.BytesAcked)\n",
        "      FROM UNNEST(tcpinfo.raw.Snapshots) AS ServerMeasurements\n",
        "    ) AS TotalBytesAcked,\n",
        "    ARRAY (\n",
        "      (\n",
        "        SELECT AS STRUCT\n",
        "          ROW_NUMBER() OVER snaps AS row_num,\n",
        "          TIMESTAMP_DIFF(SM.Timestamp, SM.FirstTimestamp, MICROSECOND) as ElapsedTime_us,\n",
        "          BytesAcked,\n",
        "          IFNULL(TIMESTAMP_DIFF(SM.Timestamp, LAG(SM.Timestamp, 1) OVER snaps, MICROSECOND), 0) AS DeltaTime_us,\n",
        "          IFNULL(BytesAcked - LAG(SM.BytesAcked, 1) OVER snaps, 0) AS DeltaBytes,\n",
        "          IFNULL(SAFE_DIVIDE(((BytesAcked - LAG(SM.BytesAcked, 1) OVER snaps)/1e6)*8 , TIMESTAMP_DIFF(SM.Timestamp, LAG(SM.Timestamp, 1) OVER snaps, MICROSECOND)/1e6), 0) AS CurrAvgMbps,\n",
        "          IFNULL(SAFE_DIVIDE((BytesAcked/1e6)*8, TIMESTAMP_DIFF(SM.Timestamp, FirstTimestamp, MICROSECOND)/1e6), 0) AS CumAvgMbps\n",
        "        FROM (\n",
        "          SELECT *\n",
        "          FROM\n",
        "            (\n",
        "              (\n",
        "                SELECT\n",
        "                  ServerMeasurements.TCPInfo.BytesAcked,\n",
        "                  ServerMeasurements.Timestamp\n",
        "                  FROM\n",
        "                    UNNEST(tcpinfo.raw.Snapshots) AS ServerMeasurements\n",
        "              )\n",
        "                CROSS JOIN\n",
        "              (\n",
        "                SELECT\n",
        "                  MIN(ServerMeasurements.Timestamp) AS FirstTimestamp FROM\n",
        "                  UNNEST(tcpinfo.raw.Snapshots) AS ServerMeasurements\n",
        "              )\n",
        "            )\n",
        "        ) AS SM\n",
        "        WINDOW \n",
        "          snaps AS (ORDER BY SM.Timestamp)\n",
        "      )\n",
        "    ) AS Snapshots\n",
        "FROM\n",
        "  `measurement-lab.ndt.tcpinfo` AS tcpinfo\n",
        "INNER JOIN\n",
        "  `measurement-lab.ndt.ndt7` AS ndt7\n",
        "ON ndt7.raw.Download.UUID = tcpinfo.raw.MetaData.UUID\n",
        "INNER JOIN\n",
        "  `measurement-lab.ndt.unified_downloads` AS unified_downloads\n",
        "ON unified_downloads.a.UUID = tcpinfo.raw.MetaData.UUID\n",
        "  WHERE\n",
        "    (ndt7.date BETWEEN '{start_date}' AND '{end_date}')\n",
        "    AND (tcpinfo.date BETWEEN '{start_date}' AND '{end_date}')\n",
        "    AND (unified_downloads.date BETWEEN '{start_date}' AND '{end_date}')\n",
        "    AND ndt7.a.TestTime >= TIMESTAMP('{start_time}')\n",
        "    AND ndt7.a.TestTime <= TIMESTAMP('{end_time}')\n",
        "),\n",
        "ground_truth AS (\n",
        "  SELECT\n",
        "    client.IP AS ClientIP,\n",
        "    APPROX_QUANTILES(a.MeanThroughputMbps, 100)[OFFSET(95)] AS p95_MeanThroughputMbps,\n",
        "    COUNT(*) AS NumTests\n",
        "  FROM\n",
        "    `measurement-lab.ndt.unified_downloads`\n",
        "  WHERE\n",
        "    (date BETWEEN '{start_date}' AND '{end_date}')\n",
        "    AND a.TestTime >= TIMESTAMP('{start_time}')\n",
        "    AND a.TestTime <= TIMESTAMP('{end_time}')\n",
        "  GROUP BY\n",
        "    client.IP\n",
        "),\n",
        "filtered_clientIPs AS (\n",
        "  WITH\n",
        "    tests_per_day AS (\n",
        "      SELECT\n",
        "        client.IP AS ClientIP,\n",
        "        TIMESTAMP_TRUNC(a.TestTime, DAY) AS TestTime,\n",
        "        COUNT(*) AS daily_tests\n",
        "      FROM\n",
        "        `measurement-lab.ndt.unified_downloads`\n",
        "      WHERE\n",
        "        (date BETWEEN '{start_date}' AND '{end_date}')\n",
        "        AND a.TestTime >= TIMESTAMP('{start_time}')\n",
        "        AND a.TestTime <= TIMESTAMP('{end_time}')\n",
        "      GROUP BY\n",
        "        1, 2\n",
        "    ),\n",
        "    threshold_days AS (\n",
        "      SELECT\n",
        "        ClientIP,\n",
        "        COUNTIF(daily_tests >= {min_daily_datapoints}) AS days_above_threshold\n",
        "      FROM\n",
        "        tests_per_day\n",
        "      GROUP BY\n",
        "        1\n",
        "    )\n",
        "  SELECT\n",
        "    ClientIP\n",
        "  FROM\n",
        "    threshold_days\n",
        "  WHERE\n",
        "    days_above_threshold >= {num_days}\n",
        "),\n",
        "{get_bytes_sample(10)},\n",
        "{get_bytes_sample(40)},\n",
        "{get_bytes_sample(100)},\n",
        "{get_bytes_sample(32)},\n",
        "{get_bytes_sample(125)},\n",
        "{get_bytes_sample(375)},\n",
        "{get_bytes_sample(625)},\n",
        "{get_bytes_sample(1250)}\n",
        "\n",
        "SELECT\n",
        "  preprocessed.UUID,\n",
        "  preprocessed.ClientIP,\n",
        "  Site,\n",
        "  TestTime,\n",
        "  TotalBytesAcked,\n",
        "  Duration_s,\n",
        "  ReturnedThroughputMbps,\n",
        "  p95_MeanThroughputMbps,\n",
        "  Cum_Estimates_{10}MB,\n",
        "  Cum_Estimates_{40}MB,\n",
        "  Cum_Estimates_{100}MB,\n",
        "  Cum_Estimates_{32}MB,\n",
        "  Cum_Estimates_{125}MB,\n",
        "  Cum_Estimates_{375}MB,\n",
        "  Cum_Estimates_{625}MB,\n",
        "  Cum_Estimates_{1250}MB,\n",
        "  ground_truth.NumTests\n",
        "FROM\n",
        "  preprocessed\n",
        "INNER JOIN\n",
        "  filtered_clientIPs\n",
        "ON\n",
        "  filtered_clientIPs.ClientIP = preprocessed.ClientIP\n",
        "INNER JOIN\n",
        "  ground_truth\n",
        "ON\n",
        "  ground_truth.ClientIP = preprocessed.ClientIP\n",
        "{bytes_inner_join(10)}\n",
        "{bytes_inner_join(40)}\n",
        "{bytes_inner_join(100)}\n",
        "{bytes_inner_join(32)}\n",
        "{bytes_inner_join(125)}\n",
        "{bytes_inner_join(375)}\n",
        "{bytes_inner_join(625)}\n",
        "{bytes_inner_join(1250)}\n",
        "WHERE\n",
        "  NumTests >= {min_datapoints}\n",
        "  AND ARRAY_LENGTH(preprocessed.Snapshots) > 0\n",
        "  AND Duration_s > {min_duration}\n",
        "'''\n",
        "\n",
        "  if to_print:\n",
        "    print(query)\n",
        "  return pd.read_gbq(query, project_id=mlab_project)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNaM9cXdQ6LV"
      },
      "source": [
        "### Simplified Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCzNpo_uQYMk"
      },
      "outputs": [],
      "source": [
        "def get_data_v3(start_date, end_date, start_time, end_time, min_daily_datapoints, num_days, min_datapoints, min_duration, to_print=False):\n",
        "  query = f'''\n",
        "WITH\n",
        "preprocessed AS (\n",
        "  SELECT\n",
        "    ndt7.raw.Download.UUID AS UUID,\n",
        "    ndt7.raw.ClientIP AS ClientIP,\n",
        "    ndt7.Server.Site AS Site,\n",
        "    ndt7.a.TestTime AS TestTime,\n",
        "    ndt7.a.MeanThroughputMbps AS ReturnedThroughputMbps,\n",
        "    TIMESTAMP_DIFF(ndt7.raw.Download.EndTime, ndt7.raw.Download.StartTime, SECOND) AS Duration_s,\n",
        "    (SELECT max(ServerMeasurements.TCPInfo.BytesAcked)\n",
        "      FROM UNNEST(tcpinfo.raw.Snapshots) AS ServerMeasurements\n",
        "    ) AS TotalBytesAcked,\n",
        "    ARRAY (\n",
        "      (\n",
        "        SELECT AS STRUCT\n",
        "          ROW_NUMBER() OVER snaps AS row_num,\n",
        "          TIMESTAMP_DIFF(SM.Timestamp, SM.FirstTimestamp, MICROSECOND) as ElapsedTime_us,\n",
        "          BytesAcked,\n",
        "          IFNULL(TIMESTAMP_DIFF(SM.Timestamp, LAG(SM.Timestamp, 1) OVER snaps, MICROSECOND), 0) AS DeltaTime_us,\n",
        "          IFNULL(BytesAcked - LAG(SM.BytesAcked, 1) OVER snaps, 0) AS DeltaBytes,\n",
        "          IFNULL(SAFE_DIVIDE(((BytesAcked - LAG(SM.BytesAcked, 1) OVER snaps)/1e6)*8 , TIMESTAMP_DIFF(SM.Timestamp, LAG(SM.Timestamp, 1) OVER snaps, MICROSECOND)/1e6), 0) AS CurrAvgMbps,\n",
        "          IFNULL(SAFE_DIVIDE((BytesAcked/1e6)*8, TIMESTAMP_DIFF(SM.Timestamp, FirstTimestamp, MICROSECOND)/1e6), 0) AS CumAvgMbps\n",
        "        FROM (\n",
        "          SELECT *\n",
        "          FROM\n",
        "            (\n",
        "              (\n",
        "                SELECT\n",
        "                  ServerMeasurements.TCPInfo.BytesAcked,\n",
        "                  ServerMeasurements.Timestamp\n",
        "                  FROM\n",
        "                    UNNEST(tcpinfo.raw.Snapshots) AS ServerMeasurements\n",
        "              )\n",
        "                CROSS JOIN\n",
        "              (\n",
        "                SELECT\n",
        "                  MIN(ServerMeasurements.Timestamp) AS FirstTimestamp FROM\n",
        "                  UNNEST(tcpinfo.raw.Snapshots) AS ServerMeasurements\n",
        "              )\n",
        "            )\n",
        "        ) AS SM\n",
        "        WINDOW \n",
        "          snaps AS (ORDER BY SM.Timestamp)\n",
        "      )\n",
        "    ) AS Snapshots\n",
        "FROM\n",
        "  `measurement-lab.ndt.tcpinfo` AS tcpinfo\n",
        "INNER JOIN\n",
        "  `measurement-lab.ndt.ndt7` AS ndt7\n",
        "ON ndt7.raw.Download.UUID = tcpinfo.raw.MetaData.UUID\n",
        "INNER JOIN\n",
        "  `measurement-lab.ndt.unified_downloads` AS unified_downloads\n",
        "ON unified_downloads.a.UUID = tcpinfo.raw.MetaData.UUID\n",
        "  WHERE\n",
        "    (ndt7.date BETWEEN '{start_date}' AND '{end_date}')\n",
        "    AND (tcpinfo.date BETWEEN '{start_date}' AND '{end_date}')\n",
        "    AND (unified_downloads.date BETWEEN '{start_date}' AND '{end_date}')\n",
        "    AND ndt7.a.TestTime >= TIMESTAMP('{start_time}')\n",
        "    AND ndt7.a.TestTime <= TIMESTAMP('{end_time}')\n",
        ")\n",
        "\n",
        "SELECT\n",
        "  preprocessed.UUID,\n",
        "  preprocessed.ClientIP,\n",
        "  Site,\n",
        "  TestTime,\n",
        "  TotalBytesAcked,\n",
        "  Duration_s,\n",
        "  ReturnedThroughputMbps,\n",
        "  Snapshots\n",
        "FROM\n",
        "  preprocessed\n",
        "WHERE\n",
        "  ARRAY_LENGTH(preprocessed.Snapshots) > 0\n",
        "  AND Duration_s > {min_duration}\n",
        "'''\n",
        "\n",
        "  if to_print:\n",
        "    print(query)\n",
        "  return pd.read_gbq(query, project_id=mlab_project)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrD7A8v5GBzK"
      },
      "source": [
        "### Make the Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLHz4ZqQGBM_"
      },
      "outputs": [],
      "source": [
        "df_v3 = get_data_v3(start_date, end_date, start_time, end_time, min_daily_datapoints, num_days, min_datapoints, min_duration, to_print=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35Au5oEwGT11"
      },
      "outputs": [],
      "source": [
        "df_v3['Cum_Estimate_full'] = (df_v3.TotalBytesAcked/df_v3.Duration_s)/1e6\n",
        "\n",
        "## Total tests\n",
        "print(f\"{len(df_v3)} tests\")\n",
        "print(f\"{len(df_v3.ClientIP.unique())} client IPs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYrQaZeRGmmd"
      },
      "source": [
        "## Bytes per test (sanity check of data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAj1MM0_Gps9"
      },
      "outputs": [],
      "source": [
        "data_dist = get_dists(df_v3, 'TotalBytesAcked')\n",
        "data_dist_lt20s = get_dists(df_v3[df_v3.Duration_s <= 20], 'TotalBytesAcked')\n",
        "data_dist_lt10s = get_dists(df_v3[df_v3.Duration_s <= 10], 'TotalBytesAcked')\n",
        "\n",
        "fig, ax = plt.subplots(dpi=300, figsize=(6,3))\n",
        "ax.plot(data_dist.TotalBytesAcked/1e6, data_dist.cdf, linewidth=2, label='all tests')\n",
        "ax.plot(data_dist_lt20s.TotalBytesAcked/1e6, data_dist_lt20s.cdf, linewidth=2, linestyle='--', label='duration <= 20s')\n",
        "ax.plot(data_dist_lt10s.TotalBytesAcked/1e6, data_dist_lt10s.cdf, linewidth=2, linestyle='--', label='duration <= 10s')\n",
        "\n",
        "ax.set_xscale('log')\n",
        "ax.set_xlim(0.1, max(df_v3.TotalBytesAcked)/1e6)\n",
        "ax.set_xlabel('Total Bytes Acked (MB)')\n",
        "ax.set_ylabel(\"CDF\")\n",
        "ax.set_title(f\"Bytes Acked (tests longer than {min_duration}s)\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiQPqnonIYlc"
      },
      "outputs": [],
      "source": [
        "print(f\"Most bytes transferred {max(df_v3.TotalBytesAcked)/1e6}MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqGPeP3lWWYK"
      },
      "source": [
        "## Time series of a single test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dklM7WD9Ybrf"
      },
      "outputs": [],
      "source": [
        "print(df_v3.keys())\n",
        "dp = df_v3\n",
        "print(\"dp.TotalBytesAcked\", dp.TotalBytesAcked[0])\n",
        "print(\"dp.Duration_s\", dp.Snapshots[0][-1][\"ElapsedTime_us\"], dp.Snapshots[1][-1][\"ElapsedTime_us\"])\n",
        "print(\"# of snapshots\", len(dp.Snapshots[0]), len(dp.Snapshots[1]))\n",
        "print(\"avg throughput:\", dp.TotalBytesAcked[0] * 8 / dp.Duration_s[0] / 1e6)\n",
        "print(\"returned mean throughput:\", dp.ReturnedThroughputMbps[0])\n",
        "dp[\"Snapshots\"][8]\n",
        "#dp[\"Snapshots\"][1][:10]\n",
        "\n",
        "# Observations\n",
        "# 1. Delta between timestamps are not uniform and not predictable.\n",
        "# 2. Total test time is not constant.\n",
        "# 3. Total number of snapshots is not constant.\n",
        "# 4. \"ReturnedThroughputMbps\" does NOT match a naive (Bytes/ElapsedTime) computation.\n",
        "#   4a. Question: is ReturnedThroughputMbps the ground truth?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99wwBI1JWT0_"
      },
      "source": [
        "### Plots of Multiple tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYza17gWWRBv"
      },
      "outputs": [],
      "source": [
        "def plot_snapshots(snapshots, returnedthroughputs, rows, cols, truncate=0, predictedthroughputs=[]):\n",
        "  plotX = rows\n",
        "  plotY = cols\n",
        "\n",
        "  figure(figsize=(plotY * 2, plotX * 2))\n",
        "\n",
        "  for i in range(plotX):\n",
        "    for j in range(plotY):\n",
        "      index = i * plotY + j\n",
        "      data = snapshots[index]\n",
        "      series = []\n",
        "      timestamps = []\n",
        "      for k, snapshot in enumerate(data):\n",
        "        if truncate > 0 and k == truncate:\n",
        "          break\n",
        "        timestamps.append(snapshot[\"ElapsedTime_us\"] / 1e6)\n",
        "        series.append(snapshot[\"CurrAvgMbps\"])\n",
        "\n",
        "      plt.subplot(plotX, plotY, index+1)\n",
        "      plt.plot(timestamps, series)\n",
        "      plt.axhline(returnedthroughputs[index], color='r')\n",
        "      if len(predictedthroughputs) > 0:\n",
        "        plt.axhline(predictedthroughputs[index], color='g')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7xoBzPPnQZt"
      },
      "outputs": [],
      "source": [
        "print(df_v3.keys())\n",
        "plot_snapshots(df_v3.Snapshots, df_v3.ReturnedThroughputMbps, 30, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4O3rDS4mji0i"
      },
      "source": [
        "# First model -- baseline\n",
        "\n",
        "The first model will be just use the first INPUT_TRUNCATE_SNAPSHOTS datapoints. If there are less than MINIMUM_SNAPSHOTS datapoints in total then the test will not be used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZxJzaI8lNQY"
      },
      "source": [
        "## Imports/Utilities for Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVYfBhhzlP3x"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.python.framework.ops import EagerTensor\n",
        "from tensorflow.python.ops.resource_variable_ops import ResourceVariable\n",
        "import time\n",
        "\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0xyVDgxgz1k"
      },
      "source": [
        "## Utilities for Creating training/dev/test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF9rKHQOggJK"
      },
      "source": [
        "### Visualizing how many snaphots each test has"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lmzk8GvQj5i0"
      },
      "outputs": [],
      "source": [
        "frequencies = []\n",
        "for snapshots in df_v3.Snapshots:\n",
        "  frequencies.append(len(snapshots))\n",
        "\n",
        "counts, bins = np.histogram(frequencies, bins=100)\n",
        "plt.stairs(counts, bins)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksGhk_HZGq1R"
      },
      "source": [
        "#### get_valid_serieses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxg_L3g3mLuL"
      },
      "outputs": [],
      "source": [
        "# INPUT_TRUNCATE_SNAPSHOTS is the number of snapshots with which we will predict\n",
        "# the final bandwidth.\n",
        "#########INPUT_TRUNCATE_SNAPSHOTS = 80\n",
        "# MINIMUM_SNAPSHOTS is an important number: it filters the dataset such that\n",
        "# only the tests with at least this many number of datapoints is given a\n",
        "# prediction.\n",
        "#\n",
        "# We can tweak this number such that only, say, tests with 50, or 70 datapoints\n",
        "# are retained as \"good tests\". In order to do that though we would need a good\n",
        "# rationale.\n",
        "#########MINIMUM_SNAPSHOTS = 80\n",
        "\n",
        "truncateNs = list(range(20, 81, 10))\n",
        "\n",
        "# get_valid_serieses returns all tests that contains the minimum number of snapshots\n",
        "def get_valid_serieses(minSnapshotsN):\n",
        "  valid_serieses = []\n",
        "  valid_returnedthroughputs = []\n",
        "  for i in range(len(df_v3.Snapshots)):\n",
        "    if len(df_v3.Snapshots[i]) >= minSnapshotsN:\n",
        "      valid_serieses.append(df_v3.Snapshots[i])\n",
        "      valid_returnedthroughputs.append(df_v3.ReturnedThroughputMbps[i])\n",
        "  return valid_serieses, valid_returnedthroughputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xbf_mUEhg89H"
      },
      "source": [
        "### Visualizing tests with `MINIMUM_SNAPSHOTS`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NkKMAbfo_oL"
      },
      "outputs": [],
      "source": [
        "# Whether to enable printing the visualizations\n",
        "# NOTE: There is a separate flag for the later result visualizations for convenience.\n",
        "plot_visualization = True\n",
        "\n",
        "if plot_visualization:\n",
        "  SNAPSHOT_TRUNC_FOR_VISUALIZATION = truncateNs[-1]\n",
        "  valid_serieses, valid_returnedthroughputs = get_valid_serieses(SNAPSHOT_TRUNC_FOR_VISUALIZATION)\n",
        "  plot_snapshots(valid_serieses, valid_returnedthroughputs, 5, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulzB8ULAhPG0"
      },
      "source": [
        "### Visualizing tests with `MINIMUM_SNAPSHOTS` and truncated with `INPUT_TRUNCATE_SNAPSHOTS`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRlDWpPTK-xe"
      },
      "outputs": [],
      "source": [
        "if plot_visualization:\n",
        "  plot_snapshots(valid_serieses, valid_returnedthroughputs, 5, 10, truncate=SNAPSHOT_TRUNC_FOR_VISUALIZATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhrR3g34Vj26"
      },
      "source": [
        "### Truncate and Create training/dev/test "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_XUIYxbnbEh"
      },
      "outputs": [],
      "source": [
        "### Normalization idea: normalize everything to have the same range, but don't\n",
        "### touch standard deviation or the mean (other than the range lower the mean\n",
        "### proportionally).\n",
        "def normalize(x, y):\n",
        "  maxes = x.max(axis=1, keepdims=True)\n",
        "  x /= maxes\n",
        "  maxes = maxes.squeeze()\n",
        "  y /= maxes\n",
        "\n",
        "  def denormalize(y_norm):\n",
        "    return y_norm * maxes\n",
        "  return x, y, denormalize\n",
        "\n",
        "def split_datasets(valid_serieses, valid_returnedthroughputs, truncateN, debug=False):\n",
        "  onedim_valid_serieses = []\n",
        "  labels = valid_returnedthroughputs\n",
        "  for series in valid_serieses:\n",
        "    onedim_series = []\n",
        "    for snapshot in series[:truncateN]:\n",
        "      onedim_series.append(snapshot[\"CurrAvgMbps\"])\n",
        "    onedim_valid_serieses.append(onedim_series)\n",
        "\n",
        "  onedim_valid_serieses = np.asarray(onedim_valid_serieses)\n",
        "  labels = np.asarray(labels).T\n",
        "\n",
        "  TrainingSetEndIndex = round(len(onedim_valid_serieses) * 0.8)\n",
        "  DevSetEndIndex = round(TrainingSetEndIndex + len(onedim_valid_serieses) * 0.1)\n",
        "  TestSetEndIndex = len(onedim_valid_serieses)\n",
        "\n",
        "  training_set = onedim_valid_serieses[:TrainingSetEndIndex]\n",
        "  training_set_labels = labels[:TrainingSetEndIndex]\n",
        "  dev_set = onedim_valid_serieses[TrainingSetEndIndex:DevSetEndIndex]\n",
        "  dev_set_labels = labels[TrainingSetEndIndex:DevSetEndIndex]\n",
        "  test_set = onedim_valid_serieses[DevSetEndIndex:]\n",
        "  test_set_labels = labels[DevSetEndIndex:]\n",
        "\n",
        "  training_set_unnorm, training_set_labels_unnorm = training_set.copy(), training_set_labels.copy()\n",
        "  dev_set_unnorm, dev_set_labels_unnorm = dev_set.copy(), dev_set_labels.copy()\n",
        "  test_set_unnorm, test_set_labels_unnorm = test_set.copy(), test_set_labels.copy()\n",
        "\n",
        "  training_set, training_set_labels, denormalize_training = normalize(training_set, training_set_labels)\n",
        "  dev_set, dev_set_labels, denormalize_dev = normalize(dev_set, dev_set_labels)\n",
        "  test_set, test_set_labels, denormalize_test = normalize(test_set, test_set_labels)\n",
        "\n",
        "  return training_set, training_set_labels, denormalize_training, training_set_labels_unnorm, dev_set, dev_set_labels, denormalize_dev, dev_set_labels_unnorm, test_set, test_set_labels, denormalize_test, test_set_labels_unnorm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDjjdEYn95qf"
      },
      "source": [
        "## Multi-level Perceptron Model (initial model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Get Data Function"
      ],
      "metadata": {
        "id": "AwJkWgUDAKHP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qF8VmOb1JhYI"
      },
      "outputs": [],
      "source": [
        "# These maps are for visualization\n",
        "valid_serieses_map, valid_returnedthroughputs_map = {}, {}\n",
        "\n",
        "def get_input(minSnapshotN, truncateN, debug=False):\n",
        "  valid_serieses, valid_returnedthroughputs = get_valid_serieses(minSnapshotN)\n",
        "  valid_serieses_map[minSnapshotN], valid_returnedthroughputs_map[minSnapshotN] = valid_serieses, valid_returnedthroughputs\n",
        "  training_set, training_set_labels, denormalize_training, training_set_labels_unnorm, dev_set, dev_set_labels, denormalize_dev, dev_set_labels_unnorm, test_set, test_set_labels, denormalize_test, test_set_labels_unnorm = split_datasets(valid_serieses, valid_returnedthroughputs, truncateN, debug=debug)\n",
        "  x_train, y_train, x_dev, y_dev, x_test, y_test = training_set, training_set_labels, dev_set, dev_set_labels, test_set, test_set_labels\n",
        "  return (\n",
        "      [x_train, y_train, x_dev, y_dev, x_test, y_test],\n",
        "      [denormalize_training, denormalize_dev, denormalize_test],\n",
        "      [training_set_labels_unnorm, dev_set_labels_unnorm, test_set_labels_unnorm]\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY2RaBZ7meoF"
      },
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHO1o-YptLvD"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-2\n",
        "num_epochs = 10\n",
        "minibatch_size = 1000\n",
        "enable_JTT = False\n",
        "JTT_upsample = 10\n",
        "JTT_cutoff_diff = 0.25\n",
        "# Note: this is not a hyperparameter, this is the plotting density. The lower\n",
        "# the number the more computation it has to run and hence the slower the training.\n",
        "epoch_spacing = 20\n",
        "\n",
        "loss = tf.keras.losses.MeanSquaredError()\n",
        "#loss = tf.keras.losses.MeanAbsoluteError()\n",
        "metrics = [tf.keras.losses.MeanSquaredError()]\n",
        "\n",
        "W, b, costs, train_acc, test_acc, train_preds, test_preds, training_set_labels_unnorm, dev_set_labels_unnorm, test_set_labels_unnorm = {}, {}, {}, {}, {}, {}, {}, {}, {}, {}\n",
        "\n",
        "history = {}\n",
        "\n",
        "denormalize_training, denormalize_dev, denormalize_test = {}, {}, {}\n",
        "\n",
        "for truncateN in truncateNs:\n",
        "  sample_weights = None\n",
        "  for trainCount in range(1, 3): # Just train twice!\n",
        "    if not enable_JTT and trainCount == 2:\n",
        "      continue\n",
        "    print(f\"truncateN: {truncateN}\")\n",
        "    data_np, denorm_funcs, unnorm_labels = get_input(truncateN, truncateN, debug=False)\n",
        "\n",
        "    x_train_np, y_train_np, x_dev_np, y_dev_np, x_test_np, y_test_np = data_np\n",
        "    denormalize_training[truncateN], denormalize_dev[truncateN], denormalize_test[truncateN] = denorm_funcs\n",
        "    training_set_labels_unnorm[truncateN], dev_set_labels_unnorm[truncateN], test_set_labels_unnorm[truncateN] = unnorm_labels\n",
        "\n",
        "    model = tf.keras.models.Sequential(\n",
        "        [tf.keras.layers.Input(shape=(truncateN,))]\n",
        "        + [ tf.keras.layers.Dense(dim, activation=tf.nn.leaky_relu) for dim in [64, 16, 1] ]\n",
        "    )\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss=loss,\n",
        "                  metrics=metrics)\n",
        "    \n",
        "    print(f\"{x_train_np.shape}, {y_train_np.shape}\")\n",
        "    history[truncateN] = model.fit(\n",
        "        x_train_np,\n",
        "        y_train_np,\n",
        "        validation_data=(x_test_np, y_test_np),\n",
        "        sample_weight=sample_weights,\n",
        "        epochs=num_epochs)\n",
        "    train_preds[truncateN] = model.predict(x_train_np).squeeze()\n",
        "    test_preds[truncateN] = model.predict(x_test_np).squeeze()\n",
        "\n",
        "    if enable_JTT and trainCount == 1: # Reallocate weights\n",
        "      diffs = np.abs(train_preds[truncateN] - y_train_np) / y_train_np\n",
        "      new_weights = np.floor(diffs + 1 - JTT_cutoff_diff) * JTT_upsample\n",
        "      new_weights += 1\n",
        "      sample_weights = new_weights\n",
        "\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Q9__f8wwaEq"
      },
      "outputs": [],
      "source": [
        "model.evaluate(x_test_np, y_test_np, return_dict=True)[\"loss\"]\n",
        "model.metrics_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z11fqn3GlLo"
      },
      "source": [
        "### Loss Plot per iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dz8-ehnT-BCx"
      },
      "outputs": [],
      "source": [
        "# Plot the train loss\n",
        "for truncateN in truncateNs:\n",
        "  plt.plot(np.squeeze(history[truncateN].history[\"loss\"]))\n",
        "  plt.ylabel('Train Loss')\n",
        "  plt.xlabel('iterations (per fives)')\n",
        "  plt.title(f\"(truncation {truncateN}) Learning rate =\" + str(learning_rate))\n",
        "  # Plot the test loss\n",
        "  plt.plot(np.squeeze(history[truncateN].history[\"val_loss\"]))\n",
        "  plt.ylabel('Test Loss')\n",
        "  plt.xlabel('iterations (per fives)')\n",
        "  plt.title(f\"(truncation {truncateN}) Learning rate =\" + str(learning_rate))\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa2UjiEjwY3J"
      },
      "source": [
        "### Debugging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VT6yXWyhzZT9"
      },
      "outputs": [],
      "source": [
        "print(test_preds[SNAPSHOT_TRUNC_FOR_VISUALIZATION].shape)\n",
        "\n",
        "# DEBUG: This is to check that these two tensors are indeed the same.\n",
        "#yl = list(y_train.as_numpy_iterator())\n",
        "#print(yl == training_set_labels)\n",
        "\n",
        "#print(yl == training_set_labels)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(training_set_labels_unnorm.shape, train_preds.shape)\n",
        "\n",
        "def print_samples(n):\n",
        "  for i in range(n):\n",
        "    print(training_set_labels_unnorm[i])\n",
        "    print(train_preds[i])\n",
        "print(\"Training set samples\")\n",
        "print_samples(20)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#print(test_set_labels_unnorm[0])\n",
        "#print(test_preds[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U1UeMBf0k_3"
      },
      "source": [
        "### Unnormalize predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvKg5AKNqTBG"
      },
      "outputs": [],
      "source": [
        "for truncateN in truncateNs:\n",
        "  train_preds[truncateN] = denormalize_training[truncateN](train_preds[truncateN])\n",
        "  test_preds[truncateN] = denormalize_test[truncateN](test_preds[truncateN])\n",
        "\n",
        "  # Shape check\n",
        "  print(train_preds[truncateN].shape)\n",
        "  print(test_preds[truncateN].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2zkbWftmVFl"
      },
      "source": [
        "## Result Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAyf-KKZKgq4"
      },
      "source": [
        "### CDF of Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkBx-p1lKgNH"
      },
      "outputs": [],
      "source": [
        "for truncateN in truncateNs:\n",
        "  print(test_preds[truncateN])\n",
        "  print(test_set_labels_unnorm[truncateN])\n",
        "  print(test_preds[truncateN].shape)\n",
        "  print(test_set_labels_unnorm[truncateN].shape)\n",
        "  differences = np.abs(test_set_labels_unnorm[truncateN] - test_preds[truncateN]) / test_set_labels_unnorm[truncateN]\n",
        "\n",
        "  freq = []\n",
        "  for p in range(101):\n",
        "    freq.append((differences < 0.01 * p).sum() / len(differences))\n",
        "\n",
        "  fig, ax = plt.subplots(dpi=300, figsize=(6, 3))\n",
        "\n",
        "  ax.set_title(f\"Accuracy CDF for Test Set at truncation {truncateN}\")\n",
        "  ax.set_xlabel(\"Percentage Accurate\")\n",
        "  ax.set_ylabel(\"Cumulative Tests\")\n",
        "  ax.plot(list(range(101)), freq, color='r', label=f\"All tests\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizations for a particular truncation"
      ],
      "metadata": {
        "id": "ioikfsBNLabS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Whether to enable printing the visualizations\n",
        "plot_visualization = True\n",
        "SNAPSHOT_TRUNC_FOR_VISUALIZATION = 50"
      ],
      "metadata": {
        "id": "AOXVrJ4894bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9kBEJzT9YEs"
      },
      "source": [
        "#### Training Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vHsfgn_746f"
      },
      "outputs": [],
      "source": [
        "if plot_visualization:\n",
        "  plot_snapshots(\n",
        "      valid_serieses_map[SNAPSHOT_TRUNC_FOR_VISUALIZATION],\n",
        "      valid_returnedthroughputs_map[SNAPSHOT_TRUNC_FOR_VISUALIZATION],\n",
        "      30, 10,\n",
        "      predictedthroughputs=train_preds[SNAPSHOT_TRUNC_FOR_VISUALIZATION])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSe5hJAe-RSZ"
      },
      "source": [
        "#### Training Set with truncation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EevzhbLH-R-p"
      },
      "outputs": [],
      "source": [
        "if plot_visualization:\n",
        "  plot_snapshots(\n",
        "      valid_serieses_map[SNAPSHOT_TRUNC_FOR_VISUALIZATION],\n",
        "      valid_returnedthroughputs_map[SNAPSHOT_TRUNC_FOR_VISUALIZATION],\n",
        "      30, 10,\n",
        "      truncate=SNAPSHOT_TRUNC_FOR_VISUALIZATION,\n",
        "      predictedthroughputs=train_preds[SNAPSHOT_TRUNC_FOR_VISUALIZATION])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4FDd0Zl8z0z"
      },
      "source": [
        "#### Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ijucj24WmtWc"
      },
      "outputs": [],
      "source": [
        "if plot_visualization:\n",
        "  plot_snapshots(\n",
        "      valid_serieses_map[SNAPSHOT_TRUNC_FOR_VISUALIZATION][-len(test_preds[SNAPSHOT_TRUNC_FOR_VISUALIZATION]):],\n",
        "      valid_returnedthroughputs_map[SNAPSHOT_TRUNC_FOR_VISUALIZATION][-len(test_preds[SNAPSHOT_TRUNC_FOR_VISUALIZATION]):],\n",
        "      30, 10,\n",
        "      predictedthroughputs=test_preds[SNAPSHOT_TRUNC_FOR_VISUALIZATION])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6E1iV08lPeO"
      },
      "source": [
        "#### Test set with truncation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIknMc3-9BwP"
      },
      "outputs": [],
      "source": [
        "if plot_visualization:\n",
        "  plot_snapshots(\n",
        "      valid_serieses_map[SNAPSHOT_TRUNC_FOR_VISUALIZATION][-len(test_preds[SNAPSHOT_TRUNC_FOR_VISUALIZATION]):],\n",
        "      valid_returnedthroughputs_map[SNAPSHOT_TRUNC_FOR_VISUALIZATION][-len(test_preds[SNAPSHOT_TRUNC_FOR_VISUALIZATION]):],\n",
        "      30, 10,\n",
        "      truncate=SNAPSHOT_TRUNC_FOR_VISUALIZATION,\n",
        "      predictedthroughputs=test_preds[SNAPSHOT_TRUNC_FOR_VISUALIZATION])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "UoyrBnCRMwp1",
        "Vf0gYZcKaXO0",
        "gZxJzaI8lNQY",
        "I0xyVDgxgz1k"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}